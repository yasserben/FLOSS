# Model settings
model = dict(
    type="DinoCLIP_Inferencer",
    model=dict(
        type="CLIP_DINOiser",
        class_names=[
            "road",
            "sidewalk",
            "building",
            "wall",
            "fence",
            "pole",
            "traffic light",
            "traffic sign",
            "vegetation",
            "terrain",
            "sky",
            "person",
            "rider",
            "car",
            "truck",
            "bus",
            "train",
            "motorcycle",
            "bicycle",
        ],
        clip_backbone=dict(
            type="MaskClip",
            clip_model="ViT-B-16",
            backbone=dict(img_size=224, patch_size=16),
            decode_head=dict(
                type="MaskClipHead",
                in_channels=768,
                text_channels=512,
                align_corners=False,
                use_templates=True,
                pretrained="laion2b_s34b_b88k",
                fuse_predictions=False,
            ),
        ),
        vit_arch="vit_base",
        vit_patch_size=16,
        enc_type_feats="v",
        gamma=0.2,
        in_dim=256,
        delta=0.99,
        feats_idx=-3,
        data_preprocessor=dict(
            type="SegDataPreProcessor",
        ),
        init_cfg=dict(
            type="Pretrained",
            checkpoint="./checkpoints/clip_dinoiser/model_weights.pth",
        ),
    ),
    data_preprocessor=dict(
        type="SegDataPreProcessor",
        bgr_to_rgb=True,
    ),
    num_classes=19,
    test_cfg=dict(mode="slide", stride=(224, 224), crop_size=(448, 448)),
    model_name="clipdinoiser",
)
